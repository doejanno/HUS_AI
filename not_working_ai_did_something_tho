import random
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import math
from collections import deque
import matplotlib.pyplot as plt

class Player:
    def __init__(self):
        self.front = [0, 0, 0, 0, 2, 2, 2, 2]
        self.back = [2, 2, 2, 2, 2, 2, 2, 2]
        self.side = self.front + self.back
        self.stone_count = sum(self.side)
        self.legal_moves = self.get_legal_moves()

    def show_side_bott(self):
        print(self.side[:8])
        ree = self.side[8:16]
        ree.reverse()
        print(ree)

    def show_side_top(self):
        print(self.side[8:16])
        ree = self.side[:8]
        ree.reverse()
        print(ree)

    def move(self, opp, st, start_stones):
        stones = self.side[st]
        stones = stones + start_stones
        if stones == 1:
            self.update_vars()
            return self.is_game_over()
        self.side[st] = 0
        st = (st + 1) % len(self.side)
        while stones != 0:
            if stones > 1:
                self.side[st] += 1
                stones -= 1
                st = (st + 1) % len(self.side)
            elif stones == 1:
                self.side[st] += 1
                stones -= 1
                if self.side[st] > 1:
                    stones = stones + self.take(opp, st)
                    self.move(opp, st, stones)
                break
        self.update_vars()
        return self.is_game_over()
    
    def take(self, opp, p_tile):
        snack = 0
        if p_tile <= 7:
            opp_tile = 7 - p_tile
            if opp.side[opp_tile] != 0:
                snack = opp.side[opp_tile]
                opp.side[opp_tile] = 0
                if opp.side[15 - opp_tile] != 0:
                    snack = snack + opp.side[15 - opp_tile]
                    opp.side[15 - opp_tile] = 0
        return snack
    
    def is_game_over(self):
        max_stones = max(self.side)
        if max_stones <= 1:
            return False
        else:
            return True

    def get_legal_moves(self):
        legal_moves_side = []
        for args in self.side:
            if args > 1:
                legal_moves_side.append(1)
            else:
                legal_moves_side.append(0)
        return legal_moves_side
    
    def update_vars(self):
        self.set_legal_moves()
        self.set_stone_count()

    def set_legal_moves(self):
        self.legal_moves = self.get_legal_moves()
    
    def set_stone_count(self):
        self.stone_count = sum(self.side)


class Board:
    def __init__(self):
        self.p1 = Player()
        self.p2 = Player()
    
    def show_board(self):
        self.p1.show_side_top()
        print()
        self.p2.show_side_bott()

    def play(self, p1_move, p2_move):
        if self.p1.move(self.p2, p1_move, 0):
            if self.p2.move(self.p1, p2_move, 0):
                return False
            else:
                return True
        else:
            return True

    def play_human_vs_ai(self, policy_net):
        p_turn = True  # True -> Human's turn
        while True:
            self.show_board()
            if p_turn:
                print("Your turn. Which tile do you move?")
                p1_move = int(input())
                if not self.p1.legal_moves[p1_move]:
                    print("Invalid move. Try again.")
                    continue
                done = self.p1.move(self.p2, p1_move, 0)
                if done:
                    print("You lost!")
                    break
                p_turn = False
            else:
                state = torch.tensor(self.p1.side + self.p2.side, dtype=torch.float32).unsqueeze(0)
                with torch.no_grad():
                    q_values = policy_net(state)
                    legal_moves = [i for i, move in enumerate(self.p2.legal_moves) if move == 1]
                    p2_move = max(legal_moves, key=lambda x: q_values[0, x].item())
                print(f"AI chooses tile {p2_move}")
                done = self.p2.move(self.p1, p2_move, 0)
                if done:
                    print("AI lost!")
                    break
                p_turn = True

class DQN(nn.Module):
    def __init__(self):
        super(DQN, self).__init__()
        self.fc1 = nn.Linear(32, 128)
        self.fc2 = nn.Linear(128, 128)
        self.fc3 = nn.Linear(128, 16)
    
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        return self.fc3(x)

class ReplayMemory:
    def __init__(self, capacity):
        self.memory = deque(maxlen=capacity)
    
    def push(self, *args):
        self.memory.append(args)
    
    def sample(self, batch_size):
        return random.sample(self.memory, batch_size)
    
    def __len__(self):
        return len(self.memory)

def select_action(state, policy_net, steps_done, EPS_START=0.9, EPS_END=0.05, EPS_DECAY=200):
    sample = random.random()
    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)
    steps_done += 1
    if sample > eps_threshold:
        with torch.no_grad():
            return policy_net(state).argmax().view(1, 1)
    else:
        return torch.tensor([[random.randrange(16)]], dtype=torch.long)

def optimize_model(policy_net, target_net, memory, optimizer, BATCH_SIZE=128, GAMMA=0.999):
    if len(memory) < BATCH_SIZE:
        return
    transitions = memory.sample(BATCH_SIZE)
    batch = list(zip(*transitions))

    state_batch = torch.cat(batch[0])
    action_batch = torch.cat(batch[1])
    reward_batch = torch.cat(batch[2])
    next_state_batch = torch.cat(batch[3])

    state_action_values = policy_net(state_batch).gather(1, action_batch)

    next_state_values = target_net(next_state_batch).max(1)[0].detach()
    expected_state_action_values = (next_state_values * GAMMA) + reward_batch

    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))

    optimizer.zero_grad()
    loss.backward()
    for param in policy_net.parameters():
        param.grad.data.clamp_(-1, 1)
    optimizer.step()

# Initialize
BATCH_SIZE = 128
GAMMA = 0.999
TARGET_UPDATE = 10

policy_net = DQN()
target_net = DQN()
target_net.load_state_dict(policy_net.state_dict())
target_net.eval()

optimizer = optim.RMSprop(policy_net.parameters())
memory = ReplayMemory(10000)

steps_done = 0

# Training loop
num_episodes = 100
episode_durations = []
episode_rewards = []

for i_episode in range(num_episodes):
    board = Board()
    state = torch.tensor(board.p1.side + board.p2.side, dtype=torch.float32).unsqueeze(0)
    total_reward = 0

    for t in range(100):
        p1_move = select_action(state, policy_net, steps_done)
        p2_move = random.randrange(16)  # Random move for Player 2
        done = board.play(p1_move.item(), p2_move)
        
        reward = torch.tensor([1.0 if not done else -1.0], dtype=torch.float32)
        total_reward += reward.item()
        
        next_state = torch.tensor(board.p1.side + board.p2.side, dtype=torch.float32).unsqueeze(0)
        
        memory.push(state, p1_move, reward, next_state)
        
        state = next_state
        
        optimize_model(policy_net, target_net, memory, optimizer)
        
        if done:
            episode_durations.append(t + 1)
            episode_rewards.append(total_reward)
            break
    
    if i_episode % TARGET_UPDATE == 0:
        target_net.load_state_dict(policy_net.state_dict())

print('Training complete')

# Plot training progress
plt.figure(figsize=(10,5))
plt.subplot(1, 2, 1)
plt.title('Episode durations')
plt.plot(episode_durations)
plt.subplot(1, 2, 2)
plt.title('Episode rewards')
plt.plot(episode_rewards)
plt.show()

# Play against the trained AI
board = Board()
board.play_human_vs_ai(policy_net)
